{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hkk9p2GJMmC3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python train.py\n",
    "\n",
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report\n",
    "#from pyimagesearch import config\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WcPSuIioNKwf"
   },
   "outputs": [],
   "source": [
    "def plot_training(H, N, plotPath):\n",
    "\t# construct a plot that plots and saves the training history\n",
    "\tplt.style.use(\"ggplot\")\n",
    "\tplt.figure()\n",
    "\tplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "\tplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "\tplt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "\tplt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "\tplt.title(\"Training Loss and Accuracy\")\n",
    "\tplt.xlabel(\"Epoch #\")\n",
    "\tplt.ylabel(\"Loss/Accuracy\")\n",
    "\tplt.legend(loc=\"lower left\")\n",
    "\tplt.savefig(plotPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRC_YS1LNooG"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "# initialize the list of class label names\n",
    "CLASSES = [\"Employees\",\"Normal_Customer\",\"VIP_Customer\"]\n",
    "BASE_PATH = r'C:\\Users\\AS5271\\Desktop\\fine-tuning-keras\\dataset'\n",
    "# set the path to the serialized model after training\n",
    "MODEL_PATH = r'C:\\Users\\AS5271\\Desktop\\fine-tuning-keras\\output\\VGG16-first.model'\n",
    " \n",
    "# define the path to the output training history plots\n",
    "UNFROZEN_PLOT_PATH = r'C:\\Users\\AS5271\\Desktop\\fine-tuning-keras\\output\\unfrozen_VGG16-first.png'\n",
    "WARMUP_PLOT_PATH = r'C:\\Users\\AS5271\\Desktop\\fine-tuning-keras\\output\\warmup_VGG16-first.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2292,
     "status": "ok",
     "timestamp": 1563126545451,
     "user": {
      "displayName": "Mukul Pratap Singh",
      "photoUrl": "https://lh5.googleusercontent.com/-oLjLwTUJqAA/AAAAAAAAAAI/AAAAAAAAABs/yKjW5P-hCBA/s64/photo.jpg",
      "userId": "10110274400788328380"
     },
     "user_tz": -330
    },
    "id": "f1ueWhmjNxTi",
    "outputId": "cc9f59eb-732e-4944-a7ea-e41881f8f372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# derive the paths to the training, validation, and testing\n",
    "# directories\n",
    "trainPath =r'C:\\Users\\AS5271\\Desktop\\fine-tuning-keras\\dataset\\training' \n",
    "valPath =r'C:\\Users\\AS5271\\Desktop\\fine-tuning-keras\\dataset\\vAL'\n",
    "testPath =r'C:\\Users\\AS5271\\Desktop\\fine-tuning-keras\\dataset\\evaluation'\n",
    "\n",
    "# determine the total number of image paths in training, validation,\n",
    "# and testing directories\n",
    "totalTrain = len(list(paths.list_images(trainPath)))\n",
    "print(totalTrain)\n",
    "totalVal = len(list(paths.list_images(valPath)))\n",
    "print(totalVal)\n",
    "totalTest = len(list(paths.list_images(testPath)))\n",
    "print(totalTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehg48R97OQTA"
   },
   "outputs": [],
   "source": [
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(\n",
    "\trotation_range=30,\n",
    "\tzoom_range=0.15,\n",
    "\twidth_shift_range=0.2,\n",
    "\theight_shift_range=0.2,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")\n",
    "\n",
    "# initialize the validation/testing data augmentation object (which\n",
    "# we'll be adding mean subtraction to)\n",
    "valAug = ImageDataGenerator()\n",
    "\n",
    "# define the ImageNet mean subtraction (in RGB order) and set the\n",
    "# the mean subtraction value for each of the data augmentation\n",
    "# objects\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "trainAug.mean = mean\n",
    "valAug.mean = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2086,
     "status": "ok",
     "timestamp": 1563126553675,
     "user": {
      "displayName": "Mukul Pratap Singh",
      "photoUrl": "https://lh5.googleusercontent.com/-oLjLwTUJqAA/AAAAAAAAAAI/AAAAAAAAABs/yKjW5P-hCBA/s64/photo.jpg",
      "userId": "10110274400788328380"
     },
     "user_tz": -330
    },
    "id": "usA4hboNO2KJ",
    "outputId": "b692714a-c872-423e-cdbf-7e7dd71014ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# initialize the training generator\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "\ttrainPath,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(224, 224),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=True,\n",
    "\tbatch_size=BATCH_SIZE)\n",
    "\n",
    "# initialize the validation generator\n",
    "valGen = valAug.flow_from_directory(\n",
    "\tvalPath,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(224, 224),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=False,\n",
    "\tbatch_size=BATCH_SIZE)\n",
    "\n",
    "# initialize the testing generator\n",
    "testGen = valAug.flow_from_directory(\n",
    "\ttestPath,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(224, 224),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=False,\n",
    "\tbatch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6f0fms4PM60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 09:46:34.751136 18652 deprecation_wrapper.py:119] From C:\\Users\\AS5271\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0717 09:46:34.766764 18652 deprecation_wrapper.py:119] From C:\\Users\\AS5271\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0717 09:46:34.766764 18652 deprecation_wrapper.py:119] From C:\\Users\\AS5271\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0717 09:46:34.784105 18652 deprecation_wrapper.py:119] From C:\\Users\\AS5271\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0717 09:46:35.033789 18652 deprecation_wrapper.py:119] From C:\\Users\\AS5271\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0717 09:46:35.034793 18652 deprecation_wrapper.py:119] From C:\\Users\\AS5271\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0717 09:46:35.218707 18652 deprecation.py:506] From C:\\Users\\AS5271\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# load the VGG16 network, ensuring the head FC layer sets are left\n",
    "# off\n",
    "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(512, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(len(CLASSES), activation=\"softmax\")(headModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_TX6NjPP3XZ"
   },
   "outputs": [],
   "source": [
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wNmyt2lLQMfF",
    "outputId": "ea4dca2f-73bb-4273-f570-49e8016e4155"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 09:46:39.267457 18652 deprecation_wrapper.py:119] From C:\\Users\\AS5271\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0717 09:46:39.343891 18652 deprecation.py:323] From C:\\Users\\AS5271\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n",
      "Epoch 1/50\n",
      "75/75 [==============================] - 463s 6s/step - loss: 7.1373 - acc: 0.4858 - val_loss: 6.7863 - val_acc: 0.5526\n",
      "Epoch 2/50\n",
      "75/75 [==============================] - 1179s 16s/step - loss: 6.2342 - acc: 0.5687 - val_loss: 7.1492 - val_acc: 0.5169\n",
      "Epoch 3/50\n",
      "75/75 [==============================] - 1188s 16s/step - loss: 5.6604 - acc: 0.6142 - val_loss: 5.0979 - val_acc: 0.6283\n",
      "Epoch 4/50\n",
      "75/75 [==============================] - 1183s 16s/step - loss: 5.1488 - acc: 0.6442 - val_loss: 6.0876 - val_acc: 0.6081\n",
      "Epoch 5/50\n",
      "75/75 [==============================] - 1183s 16s/step - loss: 4.8943 - acc: 0.6624 - val_loss: 5.4647 - val_acc: 0.6316\n",
      "Epoch 6/50\n",
      "75/75 [==============================] - 1177s 16s/step - loss: 4.3559 - acc: 0.6979 - val_loss: 6.5756 - val_acc: 0.5642\n",
      "Epoch 7/50\n",
      "75/75 [==============================] - 1191s 16s/step - loss: 4.4450 - acc: 0.6968 - val_loss: 6.7202 - val_acc: 0.5428\n",
      "Epoch 8/50\n",
      "75/75 [==============================] - 1161s 15s/step - loss: 4.7837 - acc: 0.6780 - val_loss: 6.1568 - val_acc: 0.6047\n",
      "Epoch 9/50\n",
      "75/75 [==============================] - 1118s 15s/step - loss: 4.0277 - acc: 0.7204 - val_loss: 5.3723 - val_acc: 0.6414\n",
      "Epoch 10/50\n",
      "75/75 [==============================] - 1350s 18s/step - loss: 3.8431 - acc: 0.7321 - val_loss: 6.1986 - val_acc: 0.5541\n",
      "Epoch 11/50\n",
      "75/75 [==============================] - 1436s 19s/step - loss: 3.8913 - acc: 0.7309 - val_loss: 6.9496 - val_acc: 0.5592\n",
      "Epoch 12/50\n",
      "75/75 [==============================] - 1387s 18s/step - loss: 3.7101 - acc: 0.7408 - val_loss: 6.0610 - val_acc: 0.6014\n",
      "Epoch 13/50\n",
      "75/75 [==============================] - 1234s 16s/step - loss: 3.7370 - acc: 0.7442 - val_loss: 6.1649 - val_acc: 0.5855\n",
      "Epoch 14/50\n",
      "75/75 [==============================] - 1153s 15s/step - loss: 3.6940 - acc: 0.7418 - val_loss: 5.9642 - val_acc: 0.6216\n",
      "Epoch 15/50\n",
      "75/75 [==============================] - 1179s 16s/step - loss: 3.7305 - acc: 0.7408 - val_loss: 5.3885 - val_acc: 0.6382\n",
      "Epoch 16/50\n",
      "75/75 [==============================] - 1160s 15s/step - loss: 3.4162 - acc: 0.7642 - val_loss: 5.7037 - val_acc: 0.6250\n",
      "Epoch 17/50\n",
      "75/75 [==============================] - 1135s 15s/step - loss: 2.9158 - acc: 0.7913 - val_loss: 5.2655 - val_acc: 0.6184\n",
      "Epoch 18/50\n",
      "75/75 [==============================] - 973s 13s/step - loss: 3.3245 - acc: 0.7679 - val_loss: 6.2961 - val_acc: 0.5912\n",
      "Epoch 19/50\n",
      "75/75 [==============================] - 984s 13s/step - loss: 2.7496 - acc: 0.8000 - val_loss: 6.0606 - val_acc: 0.5921\n",
      "Epoch 20/50\n",
      "75/75 [==============================] - 967s 13s/step - loss: 3.0945 - acc: 0.7842 - val_loss: 5.8942 - val_acc: 0.6115\n",
      "Epoch 21/50\n",
      "75/75 [==============================] - 961s 13s/step - loss: 2.7684 - acc: 0.8058 - val_loss: 5.0378 - val_acc: 0.6382\n",
      "Epoch 22/50\n",
      "75/75 [==============================] - 785s 10s/step - loss: 2.4299 - acc: 0.8266 - val_loss: 5.0056 - val_acc: 0.6419\n",
      "Epoch 23/50\n",
      "75/75 [==============================] - 793s 11s/step - loss: 2.4335 - acc: 0.8246 - val_loss: 4.6653 - val_acc: 0.6678\n",
      "Epoch 24/50\n",
      "75/75 [==============================] - 775s 10s/step - loss: 2.5071 - acc: 0.8192 - val_loss: 5.4301 - val_acc: 0.6351\n",
      "Epoch 25/50\n",
      "75/75 [==============================] - 784s 10s/step - loss: 2.2165 - acc: 0.8442 - val_loss: 5.4522 - val_acc: 0.6250\n",
      "Epoch 26/50\n",
      "75/75 [==============================] - 784s 10s/step - loss: 1.9943 - acc: 0.8492 - val_loss: 5.4507 - val_acc: 0.6318\n",
      "Epoch 27/50\n",
      "75/75 [==============================] - 786s 10s/step - loss: 1.9025 - acc: 0.8583 - val_loss: 5.1190 - val_acc: 0.6480\n",
      "Epoch 28/50\n",
      "75/75 [==============================] - 774s 10s/step - loss: 1.8672 - acc: 0.8658 - val_loss: 4.9513 - val_acc: 0.6486\n",
      "Epoch 29/50\n",
      "75/75 [==============================] - 782s 10s/step - loss: 1.9465 - acc: 0.8561 - val_loss: 5.1122 - val_acc: 0.6447\n",
      "Epoch 30/50\n",
      "75/75 [==============================] - 777s 10s/step - loss: 1.7702 - acc: 0.8658 - val_loss: 6.3085 - val_acc: 0.5676\n",
      "Epoch 31/50\n",
      "75/75 [==============================] - 782s 10s/step - loss: 1.9143 - acc: 0.8584 - val_loss: 5.3861 - val_acc: 0.6184\n",
      "Epoch 32/50\n",
      "75/75 [==============================] - 780s 10s/step - loss: 1.8182 - acc: 0.8637 - val_loss: 6.2115 - val_acc: 0.5777\n",
      "Epoch 33/50\n",
      "75/75 [==============================] - 786s 10s/step - loss: 1.6682 - acc: 0.8774 - val_loss: 4.7855 - val_acc: 0.6513\n",
      "Epoch 34/50\n",
      "75/75 [==============================] - 781s 10s/step - loss: 1.6463 - acc: 0.8724 - val_loss: 5.9963 - val_acc: 0.5980\n",
      "Epoch 35/50\n",
      "75/75 [==============================] - 781s 10s/step - loss: 1.5817 - acc: 0.8796 - val_loss: 5.3337 - val_acc: 0.6414\n",
      "Epoch 36/50\n",
      "75/75 [==============================] - 665s 9s/step - loss: 1.5131 - acc: 0.8796 - val_loss: 5.4197 - val_acc: 0.6419\n",
      "Epoch 37/50\n",
      "75/75 [==============================] - 627s 8s/step - loss: 1.4892 - acc: 0.8846 - val_loss: 4.9075 - val_acc: 0.6711\n",
      "Epoch 38/50\n",
      "75/75 [==============================] - 625s 8s/step - loss: 1.4476 - acc: 0.8859 - val_loss: 4.6851 - val_acc: 0.6791\n",
      "Epoch 39/50\n",
      "75/75 [==============================] - 629s 8s/step - loss: 1.3342 - acc: 0.8917 - val_loss: 4.3649 - val_acc: 0.6809\n",
      "Epoch 40/50\n",
      "75/75 [==============================] - 619s 8s/step - loss: 1.1761 - acc: 0.9021 - val_loss: 5.2639 - val_acc: 0.6216\n",
      "Epoch 41/50\n",
      "75/75 [==============================] - 627s 8s/step - loss: 1.0773 - acc: 0.9108 - val_loss: 4.9448 - val_acc: 0.6382\n",
      "Epoch 42/50\n",
      "75/75 [==============================] - 624s 8s/step - loss: 0.9945 - acc: 0.9158 - val_loss: 5.1557 - val_acc: 0.6284\n",
      "Epoch 43/50\n",
      "75/75 [==============================] - 629s 8s/step - loss: 0.9512 - acc: 0.9112 - val_loss: 5.3392 - val_acc: 0.6283\n",
      "Epoch 44/50\n",
      "75/75 [==============================] - 619s 8s/step - loss: 0.9493 - acc: 0.9150 - val_loss: 5.6191 - val_acc: 0.6149\n",
      "Epoch 45/50\n",
      "75/75 [==============================] - 627s 8s/step - loss: 0.7550 - acc: 0.9254 - val_loss: 4.7122 - val_acc: 0.6645\n",
      "Epoch 46/50\n",
      "75/75 [==============================] - 622s 8s/step - loss: 0.8254 - acc: 0.9213 - val_loss: 5.2937 - val_acc: 0.6149\n",
      "Epoch 47/50\n",
      "75/75 [==============================] - 626s 8s/step - loss: 0.7774 - acc: 0.9234 - val_loss: 4.8606 - val_acc: 0.6480\n",
      "Epoch 48/50\n",
      "75/75 [==============================] - 624s 8s/step - loss: 0.7859 - acc: 0.9267 - val_loss: 4.5972 - val_acc: 0.6182\n",
      "Epoch 49/50\n",
      "75/75 [==============================] - 632s 8s/step - loss: 0.7914 - acc: 0.9213 - val_loss: 4.9223 - val_acc: 0.6184\n",
      "Epoch 50/50\n",
      "75/75 [==============================] - 629s 8s/step - loss: 0.7532 - acc: 0.9242 - val_loss: 4.9152 - val_acc: 0.6554\n",
      "[INFO] evaluating after fine-tuning network head...\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Employees       0.76      0.60      0.67        42\n",
      "Normal_Customer       0.65      0.71      0.68        42\n",
      "   VIP_Customer       0.59      0.67      0.62        36\n",
      "\n",
      "      micro avg       0.66      0.66      0.66       120\n",
      "      macro avg       0.67      0.66      0.66       120\n",
      "   weighted avg       0.67      0.66      0.66       120\n",
      "\n",
      "<keras.engine.input_layer.InputLayer object at 0x0000020847086C50>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000020844A75D68>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000020847128208>: False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x00000208470A2CC0>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000208470A2D68>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000208474B9E48>: False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x00000208475D2CF8>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000208475D2A20>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000208477D37F0>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000020847545BE0>: False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x00000208477EBDD8>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000208477EBAC8>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000020847142278>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002084715AF60>: False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000020847176E48>: False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002084730FC88>: True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002084733F400>: True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002084733F898>: True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000020847738CF8>: True\n",
      "[INFO] re-compiling model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 136s 12s/step - loss: 0.8041 - acc: 0.9290 - val_loss: 5.1340 - val_acc: 0.6042\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 121s 11s/step - loss: 1.0985 - acc: 0.9028 - val_loss: 4.6316 - val_acc: 0.6364\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 121s 11s/step - loss: 2.2309 - acc: 0.8329 - val_loss: 4.7206 - val_acc: 0.6136\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 123s 11s/step - loss: 1.8255 - acc: 0.8464 - val_loss: 8.4240 - val_acc: 0.3977\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 126s 11s/step - loss: 2.3737 - acc: 0.7843 - val_loss: 5.8055 - val_acc: 0.5625\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 124s 11s/step - loss: 1.6316 - acc: 0.8500 - val_loss: 4.1446 - val_acc: 0.6250\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 124s 11s/step - loss: 2.1339 - acc: 0.8178 - val_loss: 3.7667 - val_acc: 0.6818\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 121s 11s/step - loss: 1.5476 - acc: 0.8657 - val_loss: 4.7324 - val_acc: 0.6591\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 124s 11s/step - loss: 0.7248 - acc: 0.9064 - val_loss: 4.4206 - val_acc: 0.6562\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 121s 11s/step - loss: 1.2547 - acc: 0.8800 - val_loss: 4.7986 - val_acc: 0.6023\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 122s 11s/step - loss: 1.5900 - acc: 0.8615 - val_loss: 5.0979 - val_acc: 0.5909\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 122s 11s/step - loss: 1.8280 - acc: 0.8199 - val_loss: 5.9689 - val_acc: 0.5795\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 131s 12s/step - loss: 1.4675 - acc: 0.8324 - val_loss: 5.0653 - val_acc: 0.5833\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 121s 11s/step - loss: 3.5636 - acc: 0.7058 - val_loss: 6.1138 - val_acc: 0.5795\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 121s 11s/step - loss: 2.1814 - acc: 0.8043 - val_loss: 5.5021 - val_acc: 0.5568\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 121s 11s/step - loss: 1.5888 - acc: 0.8422 - val_loss: 4.9531 - val_acc: 0.6023\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 125s 11s/step - loss: 1.5421 - acc: 0.8215 - val_loss: 4.6971 - val_acc: 0.6354\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 122s 11s/step - loss: 1.3692 - acc: 0.8657 - val_loss: 4.5093 - val_acc: 0.6477\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 121s 11s/step - loss: 1.3643 - acc: 0.8672 - val_loss: 4.0294 - val_acc: 0.6364\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 122s 11s/step - loss: 1.2660 - acc: 0.8628 - val_loss: 6.0760 - val_acc: 0.5341\n",
      "[INFO] evaluating after fine-tuning network...\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Employees       0.56      0.55      0.55        42\n",
      "Normal_Customer       0.56      0.79      0.65        42\n",
      "   VIP_Customer       0.70      0.39      0.50        36\n",
      "\n",
      "      micro avg       0.58      0.58      0.58       120\n",
      "      macro avg       0.61      0.57      0.57       120\n",
      "   weighted avg       0.60      0.58      0.57       120\n",
      "\n",
      "[INFO] serializing network...\n"
     ]
    }
   ],
   "source": [
    "# compile our model (this needs to be done after our setting our\n",
    "# layers to being non-trainable\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=1e-4, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "# train the head of the network for a few epochs (all other layers\n",
    "# are frozen) -- this will allow the new FC layers to start to become\n",
    "# initialized with actual \"learned\" values versus pure random\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit_generator(\n",
    "\ttrainGen,\n",
    "\tsteps_per_epoch=75,\n",
    "\tvalidation_data=valGen,\n",
    "\tvalidation_steps=10,\n",
    "\tepochs=50)\n",
    "\n",
    "# reset the testing generator and evaluate the network after\n",
    "# fine-tuning just the network head\n",
    "print(\"[INFO] evaluating after fine-tuning network head...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict_generator(testGen,\n",
    "\tsteps=(totalTest // BATCH_SIZE) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(testGen.classes, predIdxs,\n",
    "\ttarget_names=testGen.class_indices.keys()))\n",
    "plot_training(H, 50, WARMUP_PLOT_PATH)\n",
    "\n",
    "# reset our data generators\n",
    "trainGen.reset()\n",
    "valGen.reset()\n",
    "\n",
    "# now that the head FC layers have been trained/initialized, lets\n",
    "# unfreeze the final set of CONV layers and make them trainable\n",
    "for layer in baseModel.layers[15:]:\n",
    "\tlayer.trainable = True\n",
    "\n",
    "# loop over the layers in the model and show which ones are trainable\n",
    "# or not\n",
    "for layer in baseModel.layers:\n",
    "\tprint(\"{}: {}\".format(layer, layer.trainable))\n",
    "\n",
    "# for the changes to the model to take affect we need to recompile\n",
    "# the model, this time using SGD with a *very* small learning rate\n",
    "print(\"[INFO] re-compiling model...\")\n",
    "opt = SGD(lr=1e-4, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "# train the model again, this time fine-tuning *both* the final set\n",
    "# of CONV layers along with our set of FC layers\n",
    "H = model.fit_generator(\n",
    "\ttrainGen,\n",
    "\tsteps_per_epoch=totalTrain // BATCH_SIZE,\n",
    "\tvalidation_data=valGen,\n",
    "\tvalidation_steps=totalVal // BATCH_SIZE,\n",
    "\tepochs=20)\n",
    "\n",
    "# reset the testing generator and then use our trained model to\n",
    "# make predictions on the data\n",
    "print(\"[INFO] evaluating after fine-tuning network...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict_generator(testGen,\n",
    "\tsteps=(totalTest // BATCH_SIZE) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(testGen.classes, predIdxs,\n",
    "\ttarget_names=testGen.class_indices.keys()))\n",
    "plot_training(H, 20, UNFROZEN_PLOT_PATH)\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4LMK4kDRdJu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "VGG16.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
